{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Electrocardiograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Electrocardiograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 Import the [`electrocardiograms.csv`](https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_Electrocardiograms_dataset.csv) dataset and display its first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>x_11</th>\n",
       "      <th>x_12</th>\n",
       "      <th>x_13</th>\n",
       "      <th>x_14</th>\n",
       "      <th>x_15</th>\n",
       "      <th>x_16</th>\n",
       "      <th>x_17</th>\n",
       "      <th>x_18</th>\n",
       "      <th>x_19</th>\n",
       "      <th>x_20</th>\n",
       "      <th>x_21</th>\n",
       "      <th>x_22</th>\n",
       "      <th>x_23</th>\n",
       "      <th>x_24</th>\n",
       "      <th>x_25</th>\n",
       "      <th>x_26</th>\n",
       "      <th>x_27</th>\n",
       "      <th>x_28</th>\n",
       "      <th>x_29</th>\n",
       "      <th>x_30</th>\n",
       "      <th>x_31</th>\n",
       "      <th>x_32</th>\n",
       "      <th>x_33</th>\n",
       "      <th>x_34</th>\n",
       "      <th>x_35</th>\n",
       "      <th>x_36</th>\n",
       "      <th>x_37</th>\n",
       "      <th>x_38</th>\n",
       "      <th>x_39</th>\n",
       "      <th>x_40</th>\n",
       "      <th>x_41</th>\n",
       "      <th>x_42</th>\n",
       "      <th>x_43</th>\n",
       "      <th>x_44</th>\n",
       "      <th>x_45</th>\n",
       "      <th>x_46</th>\n",
       "      <th>x_47</th>\n",
       "      <th>x_48</th>\n",
       "      <th>x_49</th>\n",
       "      <th>x_50</th>\n",
       "      <th>x_51</th>\n",
       "      <th>x_52</th>\n",
       "      <th>x_53</th>\n",
       "      <th>x_54</th>\n",
       "      <th>x_55</th>\n",
       "      <th>x_56</th>\n",
       "      <th>x_57</th>\n",
       "      <th>x_58</th>\n",
       "      <th>x_59</th>\n",
       "      <th>x_60</th>\n",
       "      <th>x_61</th>\n",
       "      <th>x_62</th>\n",
       "      <th>x_63</th>\n",
       "      <th>x_64</th>\n",
       "      <th>x_65</th>\n",
       "      <th>x_66</th>\n",
       "      <th>x_67</th>\n",
       "      <th>x_68</th>\n",
       "      <th>x_69</th>\n",
       "      <th>x_70</th>\n",
       "      <th>x_71</th>\n",
       "      <th>x_72</th>\n",
       "      <th>x_73</th>\n",
       "      <th>x_74</th>\n",
       "      <th>x_75</th>\n",
       "      <th>x_76</th>\n",
       "      <th>x_77</th>\n",
       "      <th>x_78</th>\n",
       "      <th>x_79</th>\n",
       "      <th>x_80</th>\n",
       "      <th>x_81</th>\n",
       "      <th>x_82</th>\n",
       "      <th>x_83</th>\n",
       "      <th>x_84</th>\n",
       "      <th>x_85</th>\n",
       "      <th>x_86</th>\n",
       "      <th>x_87</th>\n",
       "      <th>x_88</th>\n",
       "      <th>x_89</th>\n",
       "      <th>x_90</th>\n",
       "      <th>x_91</th>\n",
       "      <th>x_92</th>\n",
       "      <th>x_93</th>\n",
       "      <th>x_94</th>\n",
       "      <th>x_95</th>\n",
       "      <th>x_96</th>\n",
       "      <th>x_97</th>\n",
       "      <th>x_98</th>\n",
       "      <th>x_99</th>\n",
       "      <th>x_100</th>\n",
       "      <th>x_101</th>\n",
       "      <th>x_102</th>\n",
       "      <th>x_103</th>\n",
       "      <th>x_104</th>\n",
       "      <th>x_105</th>\n",
       "      <th>x_106</th>\n",
       "      <th>x_107</th>\n",
       "      <th>x_108</th>\n",
       "      <th>x_109</th>\n",
       "      <th>x_110</th>\n",
       "      <th>x_111</th>\n",
       "      <th>x_112</th>\n",
       "      <th>x_113</th>\n",
       "      <th>x_114</th>\n",
       "      <th>x_115</th>\n",
       "      <th>x_116</th>\n",
       "      <th>x_117</th>\n",
       "      <th>x_118</th>\n",
       "      <th>x_119</th>\n",
       "      <th>x_120</th>\n",
       "      <th>x_121</th>\n",
       "      <th>x_122</th>\n",
       "      <th>x_123</th>\n",
       "      <th>x_124</th>\n",
       "      <th>x_125</th>\n",
       "      <th>x_126</th>\n",
       "      <th>x_127</th>\n",
       "      <th>x_128</th>\n",
       "      <th>x_129</th>\n",
       "      <th>x_130</th>\n",
       "      <th>x_131</th>\n",
       "      <th>x_132</th>\n",
       "      <th>x_133</th>\n",
       "      <th>x_134</th>\n",
       "      <th>x_135</th>\n",
       "      <th>x_136</th>\n",
       "      <th>x_137</th>\n",
       "      <th>x_138</th>\n",
       "      <th>x_139</th>\n",
       "      <th>x_140</th>\n",
       "      <th>x_141</th>\n",
       "      <th>x_142</th>\n",
       "      <th>x_143</th>\n",
       "      <th>x_144</th>\n",
       "      <th>x_145</th>\n",
       "      <th>x_146</th>\n",
       "      <th>x_147</th>\n",
       "      <th>x_148</th>\n",
       "      <th>x_149</th>\n",
       "      <th>x_150</th>\n",
       "      <th>x_151</th>\n",
       "      <th>x_152</th>\n",
       "      <th>x_153</th>\n",
       "      <th>x_154</th>\n",
       "      <th>x_155</th>\n",
       "      <th>x_156</th>\n",
       "      <th>x_157</th>\n",
       "      <th>x_158</th>\n",
       "      <th>x_159</th>\n",
       "      <th>x_160</th>\n",
       "      <th>x_161</th>\n",
       "      <th>x_162</th>\n",
       "      <th>x_163</th>\n",
       "      <th>x_164</th>\n",
       "      <th>x_165</th>\n",
       "      <th>x_166</th>\n",
       "      <th>x_167</th>\n",
       "      <th>x_168</th>\n",
       "      <th>x_169</th>\n",
       "      <th>x_170</th>\n",
       "      <th>x_171</th>\n",
       "      <th>x_172</th>\n",
       "      <th>x_173</th>\n",
       "      <th>x_174</th>\n",
       "      <th>x_175</th>\n",
       "      <th>x_176</th>\n",
       "      <th>x_177</th>\n",
       "      <th>x_178</th>\n",
       "      <th>x_179</th>\n",
       "      <th>x_180</th>\n",
       "      <th>x_181</th>\n",
       "      <th>x_182</th>\n",
       "      <th>x_183</th>\n",
       "      <th>x_184</th>\n",
       "      <th>x_185</th>\n",
       "      <th>x_186</th>\n",
       "      <th>x_187</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041199</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.146067</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.322097</td>\n",
       "      <td>0.363296</td>\n",
       "      <td>0.413858</td>\n",
       "      <td>0.426966</td>\n",
       "      <td>0.485019</td>\n",
       "      <td>0.511236</td>\n",
       "      <td>0.520599</td>\n",
       "      <td>0.548689</td>\n",
       "      <td>0.599251</td>\n",
       "      <td>0.606742</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>0.664794</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.852060</td>\n",
       "      <td>0.897004</td>\n",
       "      <td>0.953184</td>\n",
       "      <td>0.970037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992509</td>\n",
       "      <td>0.985019</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.823970</td>\n",
       "      <td>0.752809</td>\n",
       "      <td>0.711610</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.602996</td>\n",
       "      <td>0.576779</td>\n",
       "      <td>0.597378</td>\n",
       "      <td>0.670412</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.513109</td>\n",
       "      <td>0.423221</td>\n",
       "      <td>0.277154</td>\n",
       "      <td>0.119850</td>\n",
       "      <td>0.082397</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.039326</td>\n",
       "      <td>0.054307</td>\n",
       "      <td>0.063670</td>\n",
       "      <td>0.198502</td>\n",
       "      <td>0.303371</td>\n",
       "      <td>0.355805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.610714</td>\n",
       "      <td>0.466071</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>0.364286</td>\n",
       "      <td>0.346429</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.305357</td>\n",
       "      <td>0.308929</td>\n",
       "      <td>0.305357</td>\n",
       "      <td>0.291071</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.283929</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.255357</td>\n",
       "      <td>0.264286</td>\n",
       "      <td>0.260714</td>\n",
       "      <td>0.251786</td>\n",
       "      <td>0.241071</td>\n",
       "      <td>0.226786</td>\n",
       "      <td>0.217857</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.173214</td>\n",
       "      <td>0.164286</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.155357</td>\n",
       "      <td>0.141071</td>\n",
       "      <td>0.144643</td>\n",
       "      <td>0.155357</td>\n",
       "      <td>0.167857</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.192857</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.251786</td>\n",
       "      <td>0.255357</td>\n",
       "      <td>0.276786</td>\n",
       "      <td>0.310714</td>\n",
       "      <td>0.323214</td>\n",
       "      <td>0.323214</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.346429</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.348214</td>\n",
       "      <td>0.346429</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.341071</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.358929</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.308929</td>\n",
       "      <td>0.360714</td>\n",
       "      <td>0.455357</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>0.205357</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.048214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041071</td>\n",
       "      <td>0.101786</td>\n",
       "      <td>0.146429</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.246429</td>\n",
       "      <td>0.301786</td>\n",
       "      <td>0.351786</td>\n",
       "      <td>0.382143</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>0.398214</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.421429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.994200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951276</td>\n",
       "      <td>0.903712</td>\n",
       "      <td>0.917633</td>\n",
       "      <td>0.900232</td>\n",
       "      <td>0.803944</td>\n",
       "      <td>0.656613</td>\n",
       "      <td>0.421114</td>\n",
       "      <td>0.288863</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.269142</td>\n",
       "      <td>0.244780</td>\n",
       "      <td>0.230858</td>\n",
       "      <td>0.216937</td>\n",
       "      <td>0.209977</td>\n",
       "      <td>0.206497</td>\n",
       "      <td>0.193735</td>\n",
       "      <td>0.187935</td>\n",
       "      <td>0.179814</td>\n",
       "      <td>0.177494</td>\n",
       "      <td>0.160093</td>\n",
       "      <td>0.142691</td>\n",
       "      <td>0.133411</td>\n",
       "      <td>0.132251</td>\n",
       "      <td>0.121810</td>\n",
       "      <td>0.107889</td>\n",
       "      <td>0.106729</td>\n",
       "      <td>0.113689</td>\n",
       "      <td>0.096288</td>\n",
       "      <td>0.075406</td>\n",
       "      <td>0.066125</td>\n",
       "      <td>0.048724</td>\n",
       "      <td>0.022042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.024362</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.067285</td>\n",
       "      <td>0.112529</td>\n",
       "      <td>0.155452</td>\n",
       "      <td>0.196056</td>\n",
       "      <td>0.220418</td>\n",
       "      <td>0.241299</td>\n",
       "      <td>0.256380</td>\n",
       "      <td>0.257541</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.257541</td>\n",
       "      <td>0.262181</td>\n",
       "      <td>0.257541</td>\n",
       "      <td>0.259861</td>\n",
       "      <td>0.261021</td>\n",
       "      <td>0.269142</td>\n",
       "      <td>0.265661</td>\n",
       "      <td>0.263341</td>\n",
       "      <td>0.263341</td>\n",
       "      <td>0.271462</td>\n",
       "      <td>0.270302</td>\n",
       "      <td>0.270302</td>\n",
       "      <td>0.273782</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.271462</td>\n",
       "      <td>0.273782</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.279582</td>\n",
       "      <td>0.273782</td>\n",
       "      <td>0.277262</td>\n",
       "      <td>0.279582</td>\n",
       "      <td>0.279582</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.285383</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.280742</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.287703</td>\n",
       "      <td>0.286543</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.287703</td>\n",
       "      <td>0.291183</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.291183</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.291183</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.299304</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.299304</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.298144</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.306264</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.309745</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.302784</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.299304</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.305104</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.313225</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.309745</td>\n",
       "      <td>0.309745</td>\n",
       "      <td>0.317865</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.317865</td>\n",
       "      <td>0.319026</td>\n",
       "      <td>0.328306</td>\n",
       "      <td>0.341067</td>\n",
       "      <td>0.352668</td>\n",
       "      <td>0.37007</td>\n",
       "      <td>0.390951</td>\n",
       "      <td>0.385151</td>\n",
       "      <td>0.387471</td>\n",
       "      <td>0.37587</td>\n",
       "      <td>0.338747</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.402552</td>\n",
       "      <td>0.62181</td>\n",
       "      <td>0.790023</td>\n",
       "      <td>0.75174</td>\n",
       "      <td>0.468677</td>\n",
       "      <td>0.267981</td>\n",
       "      <td>0.349188</td>\n",
       "      <td>0.356148</td>\n",
       "      <td>0.313225</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.305104</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.302784</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.984472</td>\n",
       "      <td>0.962733</td>\n",
       "      <td>0.663043</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.100932</td>\n",
       "      <td>0.177019</td>\n",
       "      <td>0.270186</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.319876</td>\n",
       "      <td>0.316770</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.319876</td>\n",
       "      <td>0.316770</td>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.322981</td>\n",
       "      <td>0.330745</td>\n",
       "      <td>0.343168</td>\n",
       "      <td>0.355590</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.394410</td>\n",
       "      <td>0.406832</td>\n",
       "      <td>0.406832</td>\n",
       "      <td>0.411491</td>\n",
       "      <td>0.405280</td>\n",
       "      <td>0.395963</td>\n",
       "      <td>0.377329</td>\n",
       "      <td>0.377329</td>\n",
       "      <td>0.378882</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.363354</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.361801</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.363354</td>\n",
       "      <td>0.361801</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.371118</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.378882</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.371118</td>\n",
       "      <td>0.375776</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.364907</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.374224</td>\n",
       "      <td>0.371118</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.389752</td>\n",
       "      <td>0.394410</td>\n",
       "      <td>0.408385</td>\n",
       "      <td>0.416149</td>\n",
       "      <td>0.439441</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.470497</td>\n",
       "      <td>0.451863</td>\n",
       "      <td>0.459627</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.427019</td>\n",
       "      <td>0.399068</td>\n",
       "      <td>0.394410</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.354037</td>\n",
       "      <td>0.363354</td>\n",
       "      <td>0.364907</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.364907</td>\n",
       "      <td>0.388199</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.734472</td>\n",
       "      <td>0.911491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.614907</td>\n",
       "      <td>0.406832</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.349379</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.298137</td>\n",
       "      <td>0.301242</td>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.298137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.619217</td>\n",
       "      <td>0.489324</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.110320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060498</td>\n",
       "      <td>0.108541</td>\n",
       "      <td>0.108541</td>\n",
       "      <td>0.145907</td>\n",
       "      <td>0.192171</td>\n",
       "      <td>0.222420</td>\n",
       "      <td>0.259786</td>\n",
       "      <td>0.279359</td>\n",
       "      <td>0.282918</td>\n",
       "      <td>0.279359</td>\n",
       "      <td>0.275801</td>\n",
       "      <td>0.281139</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.286477</td>\n",
       "      <td>0.281139</td>\n",
       "      <td>0.279359</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.291815</td>\n",
       "      <td>0.297153</td>\n",
       "      <td>0.313167</td>\n",
       "      <td>0.334520</td>\n",
       "      <td>0.357651</td>\n",
       "      <td>0.380783</td>\n",
       "      <td>0.405694</td>\n",
       "      <td>0.435943</td>\n",
       "      <td>0.464413</td>\n",
       "      <td>0.476868</td>\n",
       "      <td>0.491103</td>\n",
       "      <td>0.508897</td>\n",
       "      <td>0.501779</td>\n",
       "      <td>0.496441</td>\n",
       "      <td>0.483986</td>\n",
       "      <td>0.471530</td>\n",
       "      <td>0.457295</td>\n",
       "      <td>0.430605</td>\n",
       "      <td>0.416370</td>\n",
       "      <td>0.407473</td>\n",
       "      <td>0.386121</td>\n",
       "      <td>0.359431</td>\n",
       "      <td>0.350534</td>\n",
       "      <td>0.354093</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>0.330961</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.330961</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.313167</td>\n",
       "      <td>0.314947</td>\n",
       "      <td>0.322064</td>\n",
       "      <td>0.318505</td>\n",
       "      <td>0.313167</td>\n",
       "      <td>0.311388</td>\n",
       "      <td>0.316726</td>\n",
       "      <td>0.311388</td>\n",
       "      <td>0.302491</td>\n",
       "      <td>0.298932</td>\n",
       "      <td>0.307829</td>\n",
       "      <td>0.304270</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.298932</td>\n",
       "      <td>0.291815</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.286477</td>\n",
       "      <td>0.291815</td>\n",
       "      <td>0.297153</td>\n",
       "      <td>0.302491</td>\n",
       "      <td>0.300712</td>\n",
       "      <td>0.309609</td>\n",
       "      <td>0.323843</td>\n",
       "      <td>0.330961</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.320285</td>\n",
       "      <td>0.314947</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.281139</td>\n",
       "      <td>0.274021</td>\n",
       "      <td>0.266904</td>\n",
       "      <td>0.263345</td>\n",
       "      <td>0.261566</td>\n",
       "      <td>0.263345</td>\n",
       "      <td>0.272242</td>\n",
       "      <td>0.277580</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.354093</td>\n",
       "      <td>0.471530</td>\n",
       "      <td>0.658363</td>\n",
       "      <td>0.850534</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976868</td>\n",
       "      <td>0.542705</td>\n",
       "      <td>0.193950</td>\n",
       "      <td>0.185053</td>\n",
       "      <td>0.218861</td>\n",
       "      <td>0.224199</td>\n",
       "      <td>0.201068</td>\n",
       "      <td>0.204626</td>\n",
       "      <td>0.209964</td>\n",
       "      <td>0.201068</td>\n",
       "      <td>0.197509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_1       x_2       x_3       x_4       x_5       x_6       x_7  \\\n",
       "0  0.000000  0.041199  0.112360  0.146067  0.202247  0.322097  0.363296   \n",
       "1  1.000000  0.901786  0.760714  0.610714  0.466071  0.385714  0.364286   \n",
       "2  0.994200  1.000000  0.951276  0.903712  0.917633  0.900232  0.803944   \n",
       "3  0.984472  0.962733  0.663043  0.211180  0.000000  0.032609  0.100932   \n",
       "4  0.619217  0.489324  0.327402  0.110320  0.000000  0.060498  0.108541   \n",
       "\n",
       "        x_8       x_9      x_10      x_11      x_12      x_13      x_14  \\\n",
       "0  0.413858  0.426966  0.485019  0.511236  0.520599  0.548689  0.599251   \n",
       "1  0.346429  0.314286  0.305357  0.308929  0.305357  0.291071  0.285714   \n",
       "2  0.656613  0.421114  0.288863  0.290023  0.269142  0.244780  0.230858   \n",
       "3  0.177019  0.270186  0.313665  0.307453  0.312112  0.312112  0.313665   \n",
       "4  0.108541  0.145907  0.192171  0.222420  0.259786  0.279359  0.282918   \n",
       "\n",
       "       x_15      x_16      x_17      x_18      x_19      x_20      x_21  \\\n",
       "0  0.606742  0.640449  0.664794  0.730337  0.780899  0.852060  0.897004   \n",
       "1  0.283929  0.271429  0.255357  0.264286  0.260714  0.251786  0.241071   \n",
       "2  0.216937  0.209977  0.206497  0.193735  0.187935  0.179814  0.177494   \n",
       "3  0.315217  0.319876  0.316770  0.312112  0.313665  0.319876  0.316770   \n",
       "4  0.279359  0.275801  0.281139  0.288256  0.286477  0.281139  0.279359   \n",
       "\n",
       "       x_22      x_23      x_24      x_25      x_26      x_27      x_28  \\\n",
       "0  0.953184  0.970037  1.000000  0.992509  0.985019  0.943820  0.898876   \n",
       "1  0.226786  0.217857  0.200000  0.173214  0.164286  0.160714  0.155357   \n",
       "2  0.160093  0.142691  0.133411  0.132251  0.121810  0.107889  0.106729   \n",
       "3  0.307453  0.313665  0.315217  0.315217  0.322981  0.330745  0.343168   \n",
       "4  0.290036  0.291815  0.297153  0.313167  0.334520  0.357651  0.380783   \n",
       "\n",
       "       x_29      x_30      x_31      x_32      x_33      x_34      x_35  \\\n",
       "0  0.823970  0.752809  0.711610  0.666667  0.602996  0.576779  0.597378   \n",
       "1  0.141071  0.144643  0.155357  0.167857  0.175000  0.192857  0.223214   \n",
       "2  0.113689  0.096288  0.075406  0.066125  0.048724  0.022042  0.000000   \n",
       "3  0.355590  0.366460  0.380435  0.394410  0.406832  0.406832  0.411491   \n",
       "4  0.405694  0.435943  0.464413  0.476868  0.491103  0.508897  0.501779   \n",
       "\n",
       "       x_36      x_37      x_38      x_39      x_40      x_41      x_42  \\\n",
       "0  0.670412  0.595506  0.513109  0.423221  0.277154  0.119850  0.082397   \n",
       "1  0.251786  0.255357  0.276786  0.310714  0.323214  0.323214  0.326786   \n",
       "2  0.002320  0.024362  0.046404  0.067285  0.112529  0.155452  0.196056   \n",
       "3  0.405280  0.395963  0.377329  0.377329  0.378882  0.369565  0.363354   \n",
       "4  0.496441  0.483986  0.471530  0.457295  0.430605  0.416370  0.407473   \n",
       "\n",
       "       x_43      x_44      x_45      x_46      x_47      x_48      x_49  \\\n",
       "0  0.022472  0.039326  0.054307  0.063670  0.198502  0.303371  0.355805   \n",
       "1  0.342857  0.346429  0.339286  0.342857  0.348214  0.346429  0.335714   \n",
       "2  0.220418  0.241299  0.256380  0.257541  0.252900  0.257541  0.262181   \n",
       "3  0.366460  0.366460  0.361801  0.357143  0.366460  0.369565  0.363354   \n",
       "4  0.386121  0.359431  0.350534  0.354093  0.341637  0.330961  0.327402   \n",
       "\n",
       "       x_50      x_51      x_52      x_53      x_54      x_55      x_56  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.335714  0.339286  0.341071  0.342857  0.357143  0.358929  0.328571   \n",
       "2  0.257541  0.259861  0.261021  0.269142  0.265661  0.263341  0.263341   \n",
       "3  0.361801  0.366460  0.372671  0.371118  0.369565  0.369565  0.378882   \n",
       "4  0.330961  0.327402  0.313167  0.314947  0.322064  0.318505  0.313167   \n",
       "\n",
       "       x_57      x_58      x_59      x_60      x_61      x_62      x_63  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.308929  0.360714  0.455357  0.457143  0.366071  0.205357  0.114286   \n",
       "2  0.271462  0.270302  0.270302  0.273782  0.278422  0.278422  0.271462   \n",
       "3  0.366460  0.357143  0.371118  0.375776  0.372671  0.364907  0.369565   \n",
       "4  0.311388  0.316726  0.311388  0.302491  0.298932  0.307829  0.304270   \n",
       "\n",
       "       x_64      x_65      x_66      x_67      x_68      x_69      x_70  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.048214  0.000000  0.041071  0.101786  0.146429  0.187500  0.246429   \n",
       "2  0.273782  0.278422  0.279582  0.273782  0.277262  0.279582  0.279582   \n",
       "3  0.374224  0.371118  0.372671  0.380435  0.389752  0.394410  0.408385   \n",
       "4  0.295374  0.290036  0.298932  0.291815  0.290036  0.290036  0.295374   \n",
       "\n",
       "       x_71      x_72      x_73      x_74      x_75      x_76      x_77  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.301786  0.351786  0.382143  0.387500  0.398214  0.407143  0.407143   \n",
       "2  0.278422  0.278422  0.285383  0.283063  0.280742  0.283063  0.287703   \n",
       "3  0.416149  0.439441  0.456522  0.470497  0.451863  0.459627  0.453416   \n",
       "4  0.288256  0.290036  0.288256  0.288256  0.288256  0.286477  0.291815   \n",
       "\n",
       "       x_78      x_79      x_80      x_81      x_82      x_83      x_84  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.410714  0.421429  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.286543  0.283063  0.287703  0.291183  0.293503  0.290023  0.291183   \n",
       "3  0.427019  0.399068  0.394410  0.369565  0.354037  0.363354  0.364907   \n",
       "4  0.297153  0.302491  0.300712  0.309609  0.323843  0.330961  0.327402   \n",
       "\n",
       "       x_85      x_86      x_87      x_88      x_89      x_90      x_91  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.296984  0.294664  0.290023  0.290023  0.295824  0.291183  0.293503   \n",
       "3  0.366460  0.364907  0.388199  0.535714  0.734472  0.911491  1.000000   \n",
       "4  0.320285  0.314947  0.295374  0.281139  0.274021  0.266904  0.263345   \n",
       "\n",
       "       x_92      x_93      x_94      x_95      x_96      x_97      x_98  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.293503  0.300464  0.301624  0.296984  0.294664  0.300464  0.294664   \n",
       "3  0.919255  0.614907  0.406832  0.372671  0.349379  0.315217  0.304348   \n",
       "4  0.261566  0.263345  0.272242  0.277580  0.295374  0.354093  0.471530   \n",
       "\n",
       "       x_99     x_100     x_101     x_102     x_103     x_104     x_105  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.293503  0.299304  0.303944  0.300464  0.294664  0.299304  0.307425   \n",
       "3  0.313665  0.312112  0.304348  0.298137  0.301242  0.307453  0.298137   \n",
       "4  0.658363  0.850534  1.000000  0.976868  0.542705  0.193950  0.185053   \n",
       "\n",
       "      x_106     x_107     x_108     x_109     x_110     x_111     x_112  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.301624  0.300464  0.293503  0.303944  0.303944  0.298144  0.296984   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.218861  0.224199  0.201068  0.204626  0.209964  0.201068  0.197509   \n",
       "\n",
       "      x_113     x_114     x_115     x_116     x_117     x_118     x_119  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.303944  0.306264  0.300464  0.301624  0.309745  0.312065  0.307425   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_120     x_121     x_122     x_123     x_124     x_125     x_126  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.302784  0.310905  0.308585  0.299304  0.301624  0.307425  0.310905   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_127     x_128     x_129     x_130     x_131     x_132     x_133  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.305104  0.308585  0.313225  0.310905  0.309745  0.309745  0.317865   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_134     x_135     x_136     x_137     x_138     x_139     x_140  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.315545  0.310905  0.312065  0.317865  0.319026  0.328306  0.341067   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_141    x_142     x_143     x_144     x_145    x_146     x_147  \\\n",
       "0  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "1  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "2  0.352668  0.37007  0.390951  0.385151  0.387471  0.37587  0.338747   \n",
       "3  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "4  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "\n",
       "      x_148     x_149     x_150     x_151     x_152     x_153     x_154  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.312065  0.308585  0.312065  0.307425  0.301624  0.308585  0.307425   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_155     x_156     x_157     x_158    x_159     x_160    x_161  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "2  0.300464  0.283063  0.301624  0.402552  0.62181  0.790023  0.75174   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "\n",
       "      x_162     x_163     x_164     x_165     x_166     x_167     x_168  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.468677  0.267981  0.349188  0.356148  0.313225  0.295824  0.305104   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_169     x_170     x_171     x_172     x_173     x_174     x_175  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.301624  0.302784  0.294664  0.295824  0.300464  0.296984  0.293503   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_176     x_177     x_178     x_179     x_180     x_181  x_182  x_183  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "2  0.290023  0.296984  0.300464  0.294664  0.295824  0.301624    0.0    0.0   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "\n",
       "   x_184  x_185  x_186  x_187  target  \n",
       "0    0.0    0.0    0.0    0.0       1  \n",
       "1    0.0    0.0    0.0    0.0       1  \n",
       "2    0.0    0.0    0.0    0.0       1  \n",
       "3    0.0    0.0    0.0    0.0       1  \n",
       "4    0.0    0.0    0.0    0.0       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "data = pd.read_csv('data/electrocardiograms.csv')\n",
    "\n",
    "data.head()\n",
    "\n",
    "#just a comment nova versão lets go\n",
    "#agora vai, fé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💓 Each observation of the dataset is a sequence of measured heartbeats, taken from a patient's electrocardiogram (ECG).\n",
    "\n",
    "🎯 The target is binary and defines whether the heartbeat shows:\n",
    "* a risk of cardiovascular disease 🔴 (1)\n",
    "* or not 🟢 (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓\n",
    "\n",
    "Plot an observation of each target class to get a visual idea of what the numbers represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Questions** ❓\n",
    "\n",
    "* How many observations of at-risk heartbeats are there? Save your answer as `at_risk_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "at_risk_count = data[\"target\"].value_counts()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many observations of healthy heartbeats are there? Save your answer as `healthy_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "healthy_count = data[\"target\"].value_counts()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👩🏻‍🏫 In certain cases, the class balance is representative of the true class distribution. This is the case here: the vast majority of people actually have healthy hearts. In such case, we preserve the class distribution to train the model based on reality, and adapt our modeling approach accordingly.\n",
    "\n",
    "[Centers for Disease Control and Prevention - Heart Disease Facts](https://www.cdc.gov/heartdisease/facts.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧪 **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/cazzi/.pyenv/versions/3.10.6/envs/lewagon/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/cazzi/code/caazzi/05-ML/03-Performance-metrics/data-electrocardiograms/tests\n",
      "plugins: anyio-4.8.0, time-machine-2.16.0, asyncio-0.19.0, typeguard-2.13.3\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_class_balance.py::TestClass_balance::test_at_risk_count \u001b[32mPASSED\u001b[0m\u001b[32m      [ 50%]\u001b[0m\n",
      "test_class_balance.py::TestClass_balance::test_healthy_count \u001b[32mPASSED\u001b[0m\u001b[32m      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.26s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "💯 You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/class_balance.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed class_balance step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('class_balance',\n",
    "                         healthy = healthy_count,\n",
    "                         at_risk = at_risk_count)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (3) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎯 Your task is to **flag heartbeats that are at risk of cardiovascular diseases.**\n",
    "\n",
    "👇 Let's start by investigating the performance of a `LogisticRegression` on that task. Use a ***cross-validation to evaluate the model*** on the following metrics:\n",
    "- Accuracy\n",
    "- Recall\n",
    "- Precision\n",
    "- F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cazzi/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/cazzi/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/cazzi/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/cazzi/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/cazzi/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([1.893852  , 1.84803557, 1.85196567, 1.80586243, 2.12618828]),\n",
       " 'score_time': array([0.01547527, 0.02078676, 0.0243783 , 0.01652098, 0.02468276]),\n",
       " 'test_score': array([0.9391771 , 0.93892154, 0.93968822, 0.93841043, 0.93994378])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "y = data[\"target\"]\n",
    "X = data.drop(columns=\"target\")\n",
    "\n",
    "model = LogisticRegression()\n",
    "cv_results = cross_validate(model,X,y,cv=5)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cazzi/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred_ratio = accuracy_score(y,y_pred)\n",
    "correct_detection_ratio = precision_score(y,y_pred)\n",
    "flag_ratio = recall_score(y,y_pred)\n",
    "aggregated_metric = f1_score(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question (Ratio of correct predictions)** ❓ \n",
    "\n",
    "What is the ratio of correct predictions for this model ? Save your answer under variable name `correct_pred_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question (Ability to flag at-risk patients)** ❓ \n",
    "\n",
    "What percentage of at-risk heartbeats is the model able to flag? Save your answer under variable name `flag_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question (Ability to flag correctly)** ❓ \n",
    "\n",
    "When the model signals an at-risk heartbeat, how often is it correct? Save your answer under variable name `correct_detection_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question (Detecting as many at-risk patients as possible without too many false alarms)** ❓ \n",
    "\n",
    "What is the model's ability to flag as many at-risk heartbeats as possible while limiting false alarms?  Save your answer under variable name `aggregated_metric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧪 **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/cazzi/.pyenv/versions/3.10.6/envs/lewagon/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/cazzi/code/caazzi/05-ML/03-Performance-metrics/data-electrocardiograms/tests\n",
      "plugins: anyio-4.8.0, time-machine-2.16.0, asyncio-0.19.0, typeguard-2.13.3\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_accuracy \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_f1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_precision \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_recall \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.24s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "💯 You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/logistic_regression_evaluation.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed logistic_regression_evaluation step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('logistic_regression_evaluation',\n",
    "                         accuracy = correct_pred_ratio,\n",
    "                         recall = flag_ratio,\n",
    "                         precision = correct_detection_ratio,\n",
    "                         f1 = aggregated_metric)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ Run the following cell before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should have noticed that the model was able to predict correctly in 94 cases out of 100. \n",
      "However, it was able to capture only 35.0 % of the at-risk patients\n",
      "Why ? Let's print a confusion matrix!\n"
     ]
    }
   ],
   "source": [
    "print(f\"You should have noticed that the model was able to predict correctly in {int(round(correct_pred_ratio,2)*100)} cases out of 100. \")\n",
    "\n",
    "print(f\"However, it was able to capture only {round(flag_ratio,2)*100} % of the at-risk patients\")\n",
    "\n",
    "print(\"Why ? Let's print a confusion matrix!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 Using `ConfusionMatrixDisplay` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay)),  visualize the predictions breakdown of the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>💡 Hints</summary>\n",
    "\n",
    "    \n",
    "1. [from_estimator](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator)    \n",
    "2. [from_predictions](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions)\n",
    "    \n",
    "- Don't forget to to go back to the **Holdout method** to [`train-test-split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) your dataset and look at the confusion matrix on the test set.  \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=0)\n",
    "clf = SVC(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "SVC(random_state=0)\n",
    "predictions = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ℹ️ The confusion matrix should show that the model is influenced by the class imbalance: it predicts the heartbeats to be healthy most of the time. Due to this behaviour, the model is often correct and has a **high accuracy**. However, this also causes it to miss out on many at-risk heartbeats: it has **bad recall**...\n",
    "\n",
    "👉 This model is therefore poor at the task of **flagging at-risk observations**.\n",
    "\n",
    "❗️ Don't be fooled by the accuracy and look at the metric that corresponds to your task! ❗️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question (Classification Model Selection)** ❓ \n",
    "\n",
    "Would a default KNN classifier perform better at the task of flagging at-risk observations?\n",
    "\n",
    "Save the you answer under `best_model` as \"KNN\" or \"LogisticRegression\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(X, y)\n",
    "\n",
    "y_pred2 = neigh.predict(X)\n",
    "\n",
    "correct_pred_ratio = accuracy_score(y,y_pred2)\n",
    "correct_detection_ratio = precision_score(y,y_pred2)\n",
    "flag_ratio = recall_score(y,y_pred2)\n",
    "aggregated_metric = f1_score(y,y_pred2)\n",
    "\n",
    "best_model = \"KNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9908510094556606\n",
      "0.9662013225569435\n",
      "0.9081491712707183\n",
      "0.9362762548949806\n"
     ]
    }
   ],
   "source": [
    "print(correct_pred_ratio)\n",
    "print(correct_detection_ratio)\n",
    "print(flag_ratio)\n",
    "print(aggregated_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💪 For this ECG dataset, the KNN Classifier should have a much higher recall than the LogisticRegression and therefore is better suited for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧪 **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/cazzi/.pyenv/versions/3.10.6/envs/lewagon/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/cazzi/code/caazzi/05-ML/03-Performance-metrics/data-electrocardiograms/tests\n",
      "plugins: anyio-4.8.0, time-machine-2.16.0, asyncio-0.19.0, typeguard-2.13.3\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "test_best_model.py::TestBest_model::test_best_model \u001b[32mPASSED\u001b[0m\u001b[32m               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "💯 You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/best_model.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed best_model step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('best_model',\n",
    "                         model = best_model)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected the KNN model thanks to its higherbest recall, let's have a look at the other classification performance metrics>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question (Classification Report)** ❓\n",
    "\n",
    "Print out a [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) of the KNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> 💡 <i>Hint</i>  </summary>\n",
    "    \n",
    "* You will need to pass the predictions of the model to a `classification_report`.\n",
    "    \n",
    "* SkLearn's [`cross_val_predict`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) might help 😉\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     18117\n",
      "           1       0.97      0.91      0.94      1448\n",
      "\n",
      "    accuracy                           0.99     19565\n",
      "   macro avg       0.98      0.95      0.97     19565\n",
      "weighted avg       0.99      0.99      0.99     19565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question (Reading the report)** ❓\n",
    "\n",
    "\n",
    "Among the heartbeats predicted at-risk, what is the ratio of correct predictions ? \n",
    "\n",
    "In mathematical terms, can you read the ratio $ \\frac{TP}{TP + FP} $ in the report? What is the name of this classification metrics ? \n",
    "\n",
    "Save your answer as a float under `correct_at_risk_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "correct_at_risk_predictions = 0.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧪 **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/cazzi/.pyenv/versions/3.10.6/envs/lewagon/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/cazzi/code/caazzi/05-ML/03-Performance-metrics/data-electrocardiograms/tests\n",
      "plugins: anyio-4.8.0, time-machine-2.16.0, asyncio-0.19.0, typeguard-2.13.3\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "test_precision.py::TestPrecision::test_precision \u001b[32mPASSED\u001b[0m\u001b[32m                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "💯 You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/precision.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed precision step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('precision',\n",
    "                         precision = correct_at_risk_predictions)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question (Predicting)** ❓\n",
    "\n",
    "A patient comes to you for a second opinion because  he was told that based on his heartbeats, this patient may be at-risk.  \n",
    "\n",
    "According to your optimal model, is he at-risk or not?  \n",
    "\n",
    "Save the prediction of your model under variable name `prediction` as \"at risk\" or \"healthy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>x_11</th>\n",
       "      <th>x_12</th>\n",
       "      <th>x_13</th>\n",
       "      <th>x_14</th>\n",
       "      <th>x_15</th>\n",
       "      <th>x_16</th>\n",
       "      <th>x_17</th>\n",
       "      <th>x_18</th>\n",
       "      <th>x_19</th>\n",
       "      <th>x_20</th>\n",
       "      <th>x_21</th>\n",
       "      <th>x_22</th>\n",
       "      <th>x_23</th>\n",
       "      <th>x_24</th>\n",
       "      <th>x_25</th>\n",
       "      <th>x_26</th>\n",
       "      <th>x_27</th>\n",
       "      <th>x_28</th>\n",
       "      <th>x_29</th>\n",
       "      <th>x_30</th>\n",
       "      <th>x_31</th>\n",
       "      <th>x_32</th>\n",
       "      <th>x_33</th>\n",
       "      <th>x_34</th>\n",
       "      <th>x_35</th>\n",
       "      <th>x_36</th>\n",
       "      <th>x_37</th>\n",
       "      <th>x_38</th>\n",
       "      <th>x_39</th>\n",
       "      <th>x_40</th>\n",
       "      <th>x_41</th>\n",
       "      <th>x_42</th>\n",
       "      <th>x_43</th>\n",
       "      <th>x_44</th>\n",
       "      <th>x_45</th>\n",
       "      <th>x_46</th>\n",
       "      <th>x_47</th>\n",
       "      <th>x_48</th>\n",
       "      <th>x_49</th>\n",
       "      <th>x_50</th>\n",
       "      <th>x_51</th>\n",
       "      <th>x_52</th>\n",
       "      <th>x_53</th>\n",
       "      <th>x_54</th>\n",
       "      <th>x_55</th>\n",
       "      <th>x_56</th>\n",
       "      <th>x_57</th>\n",
       "      <th>x_58</th>\n",
       "      <th>x_59</th>\n",
       "      <th>x_60</th>\n",
       "      <th>x_61</th>\n",
       "      <th>x_62</th>\n",
       "      <th>x_63</th>\n",
       "      <th>x_64</th>\n",
       "      <th>x_65</th>\n",
       "      <th>x_66</th>\n",
       "      <th>x_67</th>\n",
       "      <th>x_68</th>\n",
       "      <th>x_69</th>\n",
       "      <th>x_70</th>\n",
       "      <th>x_71</th>\n",
       "      <th>x_72</th>\n",
       "      <th>x_73</th>\n",
       "      <th>x_74</th>\n",
       "      <th>x_75</th>\n",
       "      <th>x_76</th>\n",
       "      <th>x_77</th>\n",
       "      <th>x_78</th>\n",
       "      <th>x_79</th>\n",
       "      <th>x_80</th>\n",
       "      <th>x_81</th>\n",
       "      <th>x_82</th>\n",
       "      <th>x_83</th>\n",
       "      <th>x_84</th>\n",
       "      <th>x_85</th>\n",
       "      <th>x_86</th>\n",
       "      <th>x_87</th>\n",
       "      <th>x_88</th>\n",
       "      <th>x_89</th>\n",
       "      <th>x_90</th>\n",
       "      <th>x_91</th>\n",
       "      <th>x_92</th>\n",
       "      <th>x_93</th>\n",
       "      <th>x_94</th>\n",
       "      <th>x_95</th>\n",
       "      <th>x_96</th>\n",
       "      <th>x_97</th>\n",
       "      <th>x_98</th>\n",
       "      <th>x_99</th>\n",
       "      <th>x_100</th>\n",
       "      <th>x_101</th>\n",
       "      <th>x_102</th>\n",
       "      <th>x_103</th>\n",
       "      <th>x_104</th>\n",
       "      <th>x_105</th>\n",
       "      <th>x_106</th>\n",
       "      <th>x_107</th>\n",
       "      <th>x_108</th>\n",
       "      <th>x_109</th>\n",
       "      <th>x_110</th>\n",
       "      <th>x_111</th>\n",
       "      <th>x_112</th>\n",
       "      <th>x_113</th>\n",
       "      <th>x_114</th>\n",
       "      <th>x_115</th>\n",
       "      <th>x_116</th>\n",
       "      <th>x_117</th>\n",
       "      <th>x_118</th>\n",
       "      <th>x_119</th>\n",
       "      <th>x_120</th>\n",
       "      <th>x_121</th>\n",
       "      <th>x_122</th>\n",
       "      <th>x_123</th>\n",
       "      <th>x_124</th>\n",
       "      <th>x_125</th>\n",
       "      <th>x_126</th>\n",
       "      <th>x_127</th>\n",
       "      <th>x_128</th>\n",
       "      <th>x_129</th>\n",
       "      <th>x_130</th>\n",
       "      <th>x_131</th>\n",
       "      <th>x_132</th>\n",
       "      <th>x_133</th>\n",
       "      <th>x_134</th>\n",
       "      <th>x_135</th>\n",
       "      <th>x_136</th>\n",
       "      <th>x_137</th>\n",
       "      <th>x_138</th>\n",
       "      <th>x_139</th>\n",
       "      <th>x_140</th>\n",
       "      <th>x_141</th>\n",
       "      <th>x_142</th>\n",
       "      <th>x_143</th>\n",
       "      <th>x_144</th>\n",
       "      <th>x_145</th>\n",
       "      <th>x_146</th>\n",
       "      <th>x_147</th>\n",
       "      <th>x_148</th>\n",
       "      <th>x_149</th>\n",
       "      <th>x_150</th>\n",
       "      <th>x_151</th>\n",
       "      <th>x_152</th>\n",
       "      <th>x_153</th>\n",
       "      <th>x_154</th>\n",
       "      <th>x_155</th>\n",
       "      <th>x_156</th>\n",
       "      <th>x_157</th>\n",
       "      <th>x_158</th>\n",
       "      <th>x_159</th>\n",
       "      <th>x_160</th>\n",
       "      <th>x_161</th>\n",
       "      <th>x_162</th>\n",
       "      <th>x_163</th>\n",
       "      <th>x_164</th>\n",
       "      <th>x_165</th>\n",
       "      <th>x_166</th>\n",
       "      <th>x_167</th>\n",
       "      <th>x_168</th>\n",
       "      <th>x_169</th>\n",
       "      <th>x_170</th>\n",
       "      <th>x_171</th>\n",
       "      <th>x_172</th>\n",
       "      <th>x_173</th>\n",
       "      <th>x_174</th>\n",
       "      <th>x_175</th>\n",
       "      <th>x_176</th>\n",
       "      <th>x_177</th>\n",
       "      <th>x_178</th>\n",
       "      <th>x_179</th>\n",
       "      <th>x_180</th>\n",
       "      <th>x_181</th>\n",
       "      <th>x_182</th>\n",
       "      <th>x_183</th>\n",
       "      <th>x_184</th>\n",
       "      <th>x_185</th>\n",
       "      <th>x_186</th>\n",
       "      <th>x_187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.993197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956916</td>\n",
       "      <td>0.902494</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.709751</td>\n",
       "      <td>0.557823</td>\n",
       "      <td>0.321995</td>\n",
       "      <td>0.192744</td>\n",
       "      <td>0.147392</td>\n",
       "      <td>0.129252</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.092971</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.00907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011338</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.054422</td>\n",
       "      <td>0.092971</td>\n",
       "      <td>0.113379</td>\n",
       "      <td>0.160998</td>\n",
       "      <td>0.185941</td>\n",
       "      <td>0.208617</td>\n",
       "      <td>0.219955</td>\n",
       "      <td>0.240363</td>\n",
       "      <td>0.231293</td>\n",
       "      <td>0.226757</td>\n",
       "      <td>0.231293</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.235828</td>\n",
       "      <td>0.235828</td>\n",
       "      <td>0.24263</td>\n",
       "      <td>0.249433</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.256236</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.260771</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.260771</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.276644</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.292517</td>\n",
       "      <td>0.303855</td>\n",
       "      <td>0.321995</td>\n",
       "      <td>0.337868</td>\n",
       "      <td>0.337868</td>\n",
       "      <td>0.340136</td>\n",
       "      <td>0.319728</td>\n",
       "      <td>0.297052</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.260771</td>\n",
       "      <td>0.371882</td>\n",
       "      <td>0.639456</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.807256</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.29932</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.278912</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.251701</td>\n",
       "      <td>0.256236</td>\n",
       "      <td>0.247166</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_1       x_2  x_3       x_4       x_5       x_6       x_7       x_8  \\\n",
       "0  0.904762  0.993197  1.0  0.956916  0.902494  0.857143  0.802721  0.777778   \n",
       "\n",
       "        x_9      x_10      x_11      x_12      x_13      x_14      x_15  \\\n",
       "0  0.709751  0.557823  0.321995  0.192744  0.147392  0.129252  0.099773   \n",
       "\n",
       "       x_16      x_17      x_18      x_19      x_20      x_21      x_22  \\\n",
       "0  0.092971  0.068027  0.068027  0.061224  0.040816  0.034014  0.027211   \n",
       "\n",
       "       x_23     x_24  x_25      x_26  x_27      x_28      x_29      x_30  \\\n",
       "0  0.013605  0.00907   0.0  0.006803   0.0  0.011338  0.015873  0.031746   \n",
       "\n",
       "       x_31      x_32      x_33      x_34      x_35      x_36      x_37  \\\n",
       "0  0.054422  0.092971  0.113379  0.160998  0.185941  0.208617  0.219955   \n",
       "\n",
       "       x_38      x_39      x_40      x_41      x_42      x_43      x_44  \\\n",
       "0  0.240363  0.231293  0.226757  0.231293  0.238095  0.235828  0.235828   \n",
       "\n",
       "      x_45      x_46      x_47      x_48      x_49      x_50      x_51  \\\n",
       "0  0.24263  0.249433  0.253968  0.258503  0.258503  0.256236  0.253968   \n",
       "\n",
       "       x_52      x_53      x_54      x_55      x_56      x_57      x_58  \\\n",
       "0  0.265306  0.263039  0.272109  0.265306  0.260771  0.263039  0.267574   \n",
       "\n",
       "       x_59      x_60      x_61      x_62      x_63      x_64      x_65  \\\n",
       "0  0.267574  0.274376  0.258503  0.265306  0.263039  0.267574  0.272109   \n",
       "\n",
       "       x_66      x_67      x_68      x_69      x_70      x_71      x_72  \\\n",
       "0  0.263039  0.260771  0.274376  0.269841  0.274376  0.276644  0.269841   \n",
       "\n",
       "       x_73      x_74      x_75      x_76      x_77      x_78      x_79  \\\n",
       "0  0.267574  0.274376  0.292517  0.303855  0.321995  0.337868  0.337868   \n",
       "\n",
       "       x_80      x_81      x_82      x_83      x_84      x_85      x_86  \\\n",
       "0  0.340136  0.319728  0.297052  0.285714  0.269841  0.269841  0.274376   \n",
       "\n",
       "       x_87      x_88      x_89      x_90      x_91      x_92      x_93  \\\n",
       "0  0.269841  0.274376  0.267574  0.260771  0.371882  0.639456  0.959184   \n",
       "\n",
       "       x_94      x_95     x_96      x_97      x_98      x_99     x_100  \\\n",
       "0  0.807256  0.444444  0.29932  0.272109  0.278912  0.253968  0.258503   \n",
       "\n",
       "      x_101     x_102     x_103     x_104     x_105     x_106  x_107  x_108  \\\n",
       "0  0.251701  0.256236  0.247166  0.265306  0.265306  0.267574    0.0    0.0   \n",
       "\n",
       "   x_109  x_110  x_111  x_112  x_113  x_114  x_115  x_116  x_117  x_118  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_119  x_120  x_121  x_122  x_123  x_124  x_125  x_126  x_127  x_128  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_129  x_130  x_131  x_132  x_133  x_134  x_135  x_136  x_137  x_138  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_139  x_140  x_141  x_142  x_143  x_144  x_145  x_146  x_147  x_148  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_149  x_150  x_151  x_152  x_153  x_154  x_155  x_156  x_157  x_158  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_159  x_160  x_161  x_162  x_163  x_164  x_165  x_166  x_167  x_168  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_169  x_170  x_171  x_172  x_173  x_174  x_175  x_176  x_177  x_178  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_179  x_180  x_181  x_182  x_183  x_184  x_185  x_186  x_187  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_patient = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_Electrocardiograms_new_patient.csv')\n",
    "new_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(new_patient))\n",
    "    \n",
    "prediction = \"at risk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧪 **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/cazzi/.pyenv/versions/3.10.6/envs/lewagon/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/cazzi/code/caazzi/05-ML/03-Performance-metrics/data-electrocardiograms/tests\n",
      "plugins: anyio-4.8.0, time-machine-2.16.0, asyncio-0.19.0, typeguard-2.13.3\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "test_prediction.py::TestPrediction::test_prediction_at_risk \u001b[32mPASSED\u001b[0m\u001b[32m       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "💯 You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/prediction.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed prediction step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('prediction',\n",
    "                         prediction = prediction)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🏁 Congratulations!\n",
    "\n",
    "💾 Don't forget to git add/commit/push your notebook...\n",
    "\n",
    "🚀 ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
